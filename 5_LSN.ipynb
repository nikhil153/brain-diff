{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import nibabel as nib\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def crop_center(data, out_sp):\n",
    "    \"\"\"\n",
    "    Returns the center part of volume data.\n",
    "    crop: in_sp > out_sp\n",
    "    Example: \n",
    "    data.shape = np.random.rand(182, 218, 182)\n",
    "    out_sp = (160, 192, 160)\n",
    "    data_out = crop_center(data, out_sp)\n",
    "    \"\"\"\n",
    "    in_sp = data.shape\n",
    "    nd = np.ndim(data)\n",
    "    x_crop = int((in_sp[-1] - out_sp[-1]) / 2)\n",
    "    y_crop = int((in_sp[-2] - out_sp[-2]) / 2)\n",
    "    z_crop = int((in_sp[-3] - out_sp[-3]) / 2)\n",
    "    if nd == 3:\n",
    "        data_crop = data[x_crop:-x_crop, y_crop:-y_crop, z_crop:-z_crop]\n",
    "    elif nd == 4:\n",
    "        data_crop = data[:, x_crop:-x_crop, y_crop:-y_crop, z_crop:-z_crop]\n",
    "    else:\n",
    "        raise ('Wrong dimension! dim=%d.' % nd)\n",
    "    return data_crop\n",
    "\n",
    "class UKBBDataset(Dataset):\n",
    "    '''\n",
    "        root_dir is the UKBB imaging directory\n",
    "        img_subdir is the path to the T1 image from subject dir\n",
    "        metadata_csv is a csv with age info\n",
    "        n_samples is the size of the train set and the validation set combined\n",
    "        transform is any image transformations\n",
    "    '''\n",
    "    def __init__(self, root_dir, img_subdirs, metadata_csv, transform=None):\n",
    "        self.root_dir = root_dir  \n",
    "        self.img_subdirs = img_subdirs\n",
    "        self.metadata_csv = metadata_csv\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        ukbb_metadata = pd.read_csv(self.metadata_csv)\n",
    "        return len(ukbb_metadata)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        inputs = None\n",
    "        outputs = None\n",
    "        crop_shape = (160, 192, 160)\n",
    "        \n",
    "        # Age \n",
    "        ukbb_metadata = pd.read_csv(self.metadata_csv)   \n",
    "        eid = ukbb_metadata.loc[idx,\"eid\"]\n",
    "        age_ses2 = ukbb_metadata[ukbb_metadata[\"eid\"]==eid][\"age_at_ses2\"].values[0]\n",
    "        age_ses3 = ukbb_metadata[ukbb_metadata[\"eid\"]==eid][\"age_at_ses3\"].values[0]\n",
    "       \n",
    "        # Sample subject needs to be in the MNI space\n",
    "        subject_id = f\"sub-{eid}\"\n",
    "        ses2_subdir = img_subdirs[0]\n",
    "        ses3_subdir = img_subdirs[1]\n",
    "\n",
    "        # ses-2 image\n",
    "        subject_dir = f\"{self.root_dir}{subject_id}/{ses2_subdir}/\"\n",
    "        T1_mni = f\"{subject_dir}T1_brain_to_MNI.nii.gz\"\n",
    "        img1 = nib.load(T1_mni).get_fdata()\n",
    "        img1 = img1/img1.mean()\n",
    "        img1 = crop_center(img1, crop_shape)\n",
    "        img1 = np.expand_dims(img1,0)\n",
    "\n",
    "        # ses-3 image\n",
    "        subject_dir = f\"{self.root_dir}{subject_id}/{ses3_subdir}/\"\n",
    "        T1_mni = f\"{subject_dir}T1_brain_to_MNI.nii.gz\"\n",
    "        img2 = nib.load(T1_mni).get_fdata()\n",
    "        img2 = img2/img2.mean()\n",
    "    \n",
    "        img2 = crop_center(img2, crop_shape)\n",
    "        img2 = np.expand_dims(img2,0)\n",
    "\n",
    "        print(f\"{img1.shape}, {img2.shape}, {age_ses2,age_ses3}\")\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "    \n",
    "        inputs = (torch.tensor(img1,dtype=torch.float32), torch.tensor(img2,dtype=torch.float32))\n",
    "        outputs = (torch.from_numpy(np.array(age_ses2, dtype=np.float32)),torch.from_numpy(np.array(age_ses3, dtype=np.float32)))\n",
    "        return inputs, outputs\n",
    "\n",
    "class LSN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSN, self).__init__()\n",
    "        \n",
    "        # Conv2d(input_channels, output_channels, kernel_size)\n",
    "        self.conv1 = nn.Conv3d(1, 2, 5) \n",
    "        self.conv2 = nn.Conv3d(2, 2, 5)  \n",
    "    \n",
    "        self.bn1 = nn.BatchNorm3d(2)\n",
    "        self.bn2 = nn.BatchNorm3d(2)\n",
    "    \n",
    "        self.dropout1 = nn.Dropout(0.1)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "    \n",
    "        # number of fc nodes = out_channels*((in_dim - kernel_size + 1 )/max_pool_dim)^3\n",
    "        # crop_shape = (160, 192, 160)\n",
    "\n",
    "        # after first conv + maxpool\n",
    "        # 2*(((160 - 5 + 1)/4) * ((192 - 5 + 1)/4) * ((160 - 5 + 1)/4))\n",
    "        # = 2 * (39 * 47 * 39)\n",
    "        # after second conv + maxpool\n",
    "        # 2 * ((39 - 5 + 1)//2) * ((47 - 5 + 1)//2) * ((39 - 5 + 1)//2)\n",
    "        # = 2 * (17 * 21 * 17)\n",
    "        \n",
    "        self.fc_nodes = 2 * (17 * 21 * 17)\n",
    "        self.fc1 = nn.Linear(self.fc_nodes, 128)\n",
    "        self.fcOut = nn.Linear(128, 2)\n",
    "\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def convs(self, x):\n",
    "        # out_dim = in_dim - kernel_size + 1  \n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.max_pool3d(x, 4)\n",
    "        \n",
    "        x = F.relu(self.bn2(self.conv2(x)))        \n",
    "        x = F.max_pool3d(x, 2)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.convs(x1)\n",
    "        print(\"conv shapes\")\n",
    "        print(f\"x1 shape: {x1.shape}\")\n",
    "        x1 = x1.view(-1, self.fc_nodes)\n",
    "        print(\"flattened shapes\")\n",
    "        print(f\"x1 shape: {x1.shape}\")\n",
    "        x1 = self.sigmoid(self.fc1(x1))\n",
    "\n",
    "        x2 = self.convs(x2)\n",
    "        print(\"\\nconv shapes\")\n",
    "        print(f\"x2 shape: {x2.shape}\")\n",
    "        x2 = x2.view(-1, self.fc_nodes)\n",
    "        print(\"flattened shapes\")\n",
    "        print(f\"x2 shape: {x2.shape}\")\n",
    "        x2 = self.sigmoid(self.fc1(x2))\n",
    "\n",
    "        print(\"\\nfc shapes\")\n",
    "        print(f\"x1 shape: {x1.shape}\")\n",
    "        print(f\"x2 shape: {x2.shape}\")\n",
    "        print(f\"x1 max: {torch.max(x1)}, x2 max: {torch.max(x2)}\")\n",
    "        # x = torch.abs(x1 - x2)\n",
    "        \n",
    "        x = x1-x2\n",
    "        x = self.fcOut(x)\n",
    "        return x"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "data_dir = \"/home/nikhil/projects/brain_changes/data/ukbb/\"\n",
    "img_dir = f\"{data_dir}imaging/ukbb_test_subject/\"\n",
    "img_subdirs = [\"ses-2/non-bids/T1/\",\"ses-2/non-bids/T1/\"]\n",
    "metadata_csv = f\"{data_dir}tabular/ukbb_test_subject_metadata.csv\"\n",
    "\n",
    "ukbb_dataset = UKBBDataset(img_dir, img_subdirs, metadata_csv)\n",
    "\n",
    "batch_size = 1\n",
    "train_dataloader = DataLoader(ukbb_dataset, batch_size=batch_size, shuffle=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "model = LSN()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    model.train()\n",
    "    print(\"Starting epoch \" + str(epoch+1))\n",
    "    for inputs, outputs in train_dataloader:\n",
    "        img1 = inputs[0]\n",
    "        img2 = inputs[1]\n",
    "        age_at_ses2 = outputs[0]\n",
    "        age_at_ses3 = outputs[1]\n",
    "\n",
    "        # Forward\n",
    "        img1 = img1.to(device)\n",
    "        img2 = img2.to(device)\n",
    "        age_at_ses2 = age_at_ses2.to(device)\n",
    "        age_at_ses3 = age_at_ses3.to(device)\n",
    "        outputs = model(img1, img2)\n",
    "        print(outputs)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting epoch 1\n",
      "(1, 160, 192, 160), (1, 160, 192, 160), (53.0, 55.0)\n",
      "conv shapes\n",
      "x1 shape: torch.Size([1, 2, 17, 21, 17])\n",
      "flattened shapes\n",
      "x1 shape: torch.Size([1, 12138])\n",
      "\n",
      "conv shapes\n",
      "x2 shape: torch.Size([1, 2, 17, 21, 17])\n",
      "flattened shapes\n",
      "x2 shape: torch.Size([1, 12138])\n",
      "\n",
      "fc shapes\n",
      "x1 shape: torch.Size([1, 128])\n",
      "x2 shape: torch.Size([1, 128])\n",
      "x1 max: 0.8754773139953613, x2 max: 0.8754696846008301\n",
      "tensor([[-0.0572,  0.0684]], grad_fn=<AddmmBackward>)\n",
      "(1, 160, 192, 160), (1, 160, 192, 160), (70.0, 72.0)\n",
      "conv shapes\n",
      "x1 shape: torch.Size([1, 2, 17, 21, 17])\n",
      "flattened shapes\n",
      "x1 shape: torch.Size([1, 12138])\n",
      "\n",
      "conv shapes\n",
      "x2 shape: torch.Size([1, 2, 17, 21, 17])\n",
      "flattened shapes\n",
      "x2 shape: torch.Size([1, 12138])\n",
      "\n",
      "fc shapes\n",
      "x1 shape: torch.Size([1, 128])\n",
      "x2 shape: torch.Size([1, 128])\n",
      "x1 max: 0.863959789276123, x2 max: 0.8639587163925171\n",
      "tensor([[-0.0571,  0.0684]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "int(2*(((160 - 5 + 1)/4) * ((192 - 5 + 1)/4) * ((160 - 5 + 1)/4)))        "
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "142974"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.0",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.0 64-bit ('green_compute': venv)"
  },
  "interpreter": {
   "hash": "96e4927380308772faf387ce1ad6de9eaed4a7d7aadcf2622a8269a7d5f191c8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}