{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import nibabel as nib\n",
    "import random\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import dask.dataframe as dd\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from src.utils import *\n",
    "from src.LSN_roi import *\n",
    "\n",
    "from models import dp_loss as dpl\n",
    "from models import dp_utils as dpu\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Paths"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "project_dir = \"../\"\n",
    "data_dir = \"/home/nikhil/projects/brain_changes/data/ukbb/\"\n",
    "\n",
    "freesurfer_csv = f\"{data_dir}imaging/freesurfer/ukb47552_followup_subset.csv\"\n",
    "\n",
    "train_csv = f\"{project_dir}metadata/metadata_train.csv\"\n",
    "test_csv = f\"{project_dir}metadata/metadata_test.csv\"\n",
    "\n",
    "freesurfer_fields = f\"{project_dir}/metadata/ukbb_freesurfer_fields.txt\"\n",
    "\n",
    "summary_results_dir = \"/home/nikhil/projects/brain_changes/brain-diff/results/summary/\"\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Filter followup subject using Dask dataframe (pandas will crash)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "# import dask.dataframe as dd\n",
    "\n",
    "# # Grab eids with ses-3 data\n",
    "# freesurfer_df = pd.read_csv(freesurfer_csv,usecols=[\"eid\",\"26501-3.0\"])\n",
    "# freesurfer_eids = freesurfer_df[~freesurfer_df[\"26501-3.0\"].isna()][\"eid\"]\n",
    "\n",
    "# # Read entire CSV using dask\n",
    "# freesurfer_df = dd.read_csv(freesurfer_csv)\n",
    "# followup_freesurfer_df = freesurfer_df[freesurfer_df[\"eid\"].isin(freesurfer_eids)].compute()\n",
    "\n",
    "# # Save filtered df\n",
    "# followup_freesurfer_df.to_csv(f\"{data_dir}imaging/freesurfer/ukb47552_followup_subset.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Grab phenotype fields (e.g. Thicknes, Volume, Area)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "freesurfer_fields_df = pd.read_csv(freesurfer_fields,sep=\"\t\")\n",
    "freesurfer_fields_df[\"phenotype\"] = freesurfer_fields_df[\"Description\"].str.split(\" \",1,expand=True)[0]\n",
    "freesurfer_fields_df[\"phenotype\"] = freesurfer_fields_df[\"phenotype\"].replace({\"Mean\":\"Mean Thickness\"})\n",
    "CT_fields = freesurfer_fields_df[freesurfer_fields_df[\"phenotype\"]==\"Mean Thickness\"][\"Field ID\"]\n",
    "volume_fields = freesurfer_fields_df[freesurfer_fields_df[\"phenotype\"]==\"Volume\"][\"Field ID\"]\n",
    "\n",
    "print(f\"number of CT fields: {len(CT_fields)}, volume fields: {len(volume_fields)}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "number of CT fields: 62, volume fields: 62\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Read DKT volumes "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "pheno_fields = CT_fields # + volume_fields\n",
    "pheno_cols_ses2 = list(pheno_fields.astype(str) + \"-2.0\")\n",
    "pheno_cols_ses3 = list(pheno_fields.astype(str) + \"-3.0\")\n",
    "usecols = [\"eid\"] + pheno_cols_ses2 + pheno_cols_ses3\n",
    "\n",
    "print(f\"reading {len(usecols)} columes\")\n",
    "\n",
    "freesurfer_df = pd.read_csv(freesurfer_csv, usecols=usecols)\n",
    "\n",
    "# Remove eids with missing 2nd or 3rd ses data\n",
    "eid_missing_data = freesurfer_df[freesurfer_df.isna().any(axis=1)][\"eid\"].values\n",
    "print(f\"number participants missing 2nd or 3rd ses freesurfer data: {len(eid_missing_data)}\")\n",
    "\n",
    "freesurfer_df = freesurfer_df[~freesurfer_df[\"eid\"].isin(eid_missing_data)]\n",
    "freesurfer_eids = freesurfer_df[\"eid\"].values\n",
    "\n",
    "print(f\"available freesurfer subjects: {len(freesurfer_eids)}\")\n",
    "\n",
    "\n",
    "# scale data\n",
    "# pheno_max_val = np.max(freesurfer_df[pheno_cols_ses2 + pheno_cols_ses3].values)\n",
    "# print(f\"Max pheno val: {pheno_max_val}\")\n",
    "# freesurfer_df[pheno_cols_ses2 + pheno_cols_ses3] = freesurfer_df[pheno_cols_ses2 + pheno_cols_ses3] / pheno_max_val\n",
    "\n",
    "freesurfer_df.head()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "reading 125 columes\n",
      "number participants missing 2nd or 3rd ses freesurfer data: 63\n",
      "available freesurfer subjects: 3237\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       eid  27174-2.0  27174-3.0  27175-2.0  27175-3.0  27176-2.0  27176-3.0  \\\n",
       "0  1000635      2.786      2.874      2.910      2.852      2.275      2.307   \n",
       "1  1008391      3.191      2.875      3.080      3.037      2.273      2.143   \n",
       "2  1010129      2.329      1.870      2.836      2.798      1.995      1.943   \n",
       "3  1010994      2.785      2.581      2.671      2.603      2.060      1.819   \n",
       "4  1013774      2.963      3.191      2.617      2.856      2.035      2.050   \n",
       "\n",
       "   27177-2.0  27177-3.0  27178-2.0  ...  27293-2.0  27293-3.0  27294-2.0  \\\n",
       "0      3.389      3.379      2.836  ...      2.387      2.435      2.967   \n",
       "1      3.004      2.571      2.846  ...      2.588      2.592      3.073   \n",
       "2      3.302      3.193      2.812  ...      2.376      2.395      3.081   \n",
       "3      3.144      3.225      2.793  ...      2.513      2.416      2.919   \n",
       "4      2.751      3.385      2.830  ...      2.274      2.403      3.041   \n",
       "\n",
       "   27294-3.0  27295-2.0  27295-3.0  27296-2.0  27296-3.0  27297-2.0  27297-3.0  \n",
       "0      2.958      2.707      2.628      2.229      2.142      2.875      2.750  \n",
       "1      2.859      2.839      2.770      3.086      3.322      3.255      3.003  \n",
       "2      3.086      2.993      2.945      3.016      3.032      3.193      3.168  \n",
       "3      2.881      2.654      2.581      2.088      2.161      2.912      2.866  \n",
       "4      3.008      2.635      2.737      2.527      2.696      2.983      3.263  \n",
       "\n",
       "[5 rows x 125 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eid</th>\n",
       "      <th>27174-2.0</th>\n",
       "      <th>27174-3.0</th>\n",
       "      <th>27175-2.0</th>\n",
       "      <th>27175-3.0</th>\n",
       "      <th>27176-2.0</th>\n",
       "      <th>27176-3.0</th>\n",
       "      <th>27177-2.0</th>\n",
       "      <th>27177-3.0</th>\n",
       "      <th>27178-2.0</th>\n",
       "      <th>...</th>\n",
       "      <th>27293-2.0</th>\n",
       "      <th>27293-3.0</th>\n",
       "      <th>27294-2.0</th>\n",
       "      <th>27294-3.0</th>\n",
       "      <th>27295-2.0</th>\n",
       "      <th>27295-3.0</th>\n",
       "      <th>27296-2.0</th>\n",
       "      <th>27296-3.0</th>\n",
       "      <th>27297-2.0</th>\n",
       "      <th>27297-3.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000635</td>\n",
       "      <td>2.786</td>\n",
       "      <td>2.874</td>\n",
       "      <td>2.910</td>\n",
       "      <td>2.852</td>\n",
       "      <td>2.275</td>\n",
       "      <td>2.307</td>\n",
       "      <td>3.389</td>\n",
       "      <td>3.379</td>\n",
       "      <td>2.836</td>\n",
       "      <td>...</td>\n",
       "      <td>2.387</td>\n",
       "      <td>2.435</td>\n",
       "      <td>2.967</td>\n",
       "      <td>2.958</td>\n",
       "      <td>2.707</td>\n",
       "      <td>2.628</td>\n",
       "      <td>2.229</td>\n",
       "      <td>2.142</td>\n",
       "      <td>2.875</td>\n",
       "      <td>2.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1008391</td>\n",
       "      <td>3.191</td>\n",
       "      <td>2.875</td>\n",
       "      <td>3.080</td>\n",
       "      <td>3.037</td>\n",
       "      <td>2.273</td>\n",
       "      <td>2.143</td>\n",
       "      <td>3.004</td>\n",
       "      <td>2.571</td>\n",
       "      <td>2.846</td>\n",
       "      <td>...</td>\n",
       "      <td>2.588</td>\n",
       "      <td>2.592</td>\n",
       "      <td>3.073</td>\n",
       "      <td>2.859</td>\n",
       "      <td>2.839</td>\n",
       "      <td>2.770</td>\n",
       "      <td>3.086</td>\n",
       "      <td>3.322</td>\n",
       "      <td>3.255</td>\n",
       "      <td>3.003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1010129</td>\n",
       "      <td>2.329</td>\n",
       "      <td>1.870</td>\n",
       "      <td>2.836</td>\n",
       "      <td>2.798</td>\n",
       "      <td>1.995</td>\n",
       "      <td>1.943</td>\n",
       "      <td>3.302</td>\n",
       "      <td>3.193</td>\n",
       "      <td>2.812</td>\n",
       "      <td>...</td>\n",
       "      <td>2.376</td>\n",
       "      <td>2.395</td>\n",
       "      <td>3.081</td>\n",
       "      <td>3.086</td>\n",
       "      <td>2.993</td>\n",
       "      <td>2.945</td>\n",
       "      <td>3.016</td>\n",
       "      <td>3.032</td>\n",
       "      <td>3.193</td>\n",
       "      <td>3.168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1010994</td>\n",
       "      <td>2.785</td>\n",
       "      <td>2.581</td>\n",
       "      <td>2.671</td>\n",
       "      <td>2.603</td>\n",
       "      <td>2.060</td>\n",
       "      <td>1.819</td>\n",
       "      <td>3.144</td>\n",
       "      <td>3.225</td>\n",
       "      <td>2.793</td>\n",
       "      <td>...</td>\n",
       "      <td>2.513</td>\n",
       "      <td>2.416</td>\n",
       "      <td>2.919</td>\n",
       "      <td>2.881</td>\n",
       "      <td>2.654</td>\n",
       "      <td>2.581</td>\n",
       "      <td>2.088</td>\n",
       "      <td>2.161</td>\n",
       "      <td>2.912</td>\n",
       "      <td>2.866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1013774</td>\n",
       "      <td>2.963</td>\n",
       "      <td>3.191</td>\n",
       "      <td>2.617</td>\n",
       "      <td>2.856</td>\n",
       "      <td>2.035</td>\n",
       "      <td>2.050</td>\n",
       "      <td>2.751</td>\n",
       "      <td>3.385</td>\n",
       "      <td>2.830</td>\n",
       "      <td>...</td>\n",
       "      <td>2.274</td>\n",
       "      <td>2.403</td>\n",
       "      <td>3.041</td>\n",
       "      <td>3.008</td>\n",
       "      <td>2.635</td>\n",
       "      <td>2.737</td>\n",
       "      <td>2.527</td>\n",
       "      <td>2.696</td>\n",
       "      <td>2.983</td>\n",
       "      <td>3.263</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 125 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "train_df = pd.read_csv(train_csv)\n",
    "train_eids = train_df[\"eid\"]\n",
    "train_eids_avail = set(train_eids) & set(freesurfer_eids)\n",
    "train_df = pd.merge(train_df, freesurfer_df, on=\"eid\", how=\"inner\")\n",
    "\n",
    "test_df = pd.read_csv(test_csv)\n",
    "test_eids = test_df[\"eid\"]\n",
    "test_eids_avail = set(test_eids) & set(freesurfer_eids)\n",
    "test_df = pd.merge(test_df, freesurfer_df, on=\"eid\", how=\"inner\")\n",
    "\n",
    "print(f\"train samples: {len(train_eids)}, freesurfer data available: {len(train_eids_avail)}, overlap: {len(train_df)}\")\n",
    "print(f\"test samples: {len(test_eids)}, freesurfer data available: {len(test_eids_avail)}, overlap: {len(test_df)}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train samples: 2145, freesurfer data available: 1909, overlap: 1909\n",
      "test samples: 1057, freesurfer data available: 958, overlap: 958\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "np.hstack([train_df[\"age_at_ses3\"].values, test_df[\"age_at_ses3\"].values]).mean(), np.hstack([train_df[\"age_at_ses3\"].values, test_df[\"age_at_ses3\"].values]).std()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(64.80013951866061, 7.208743672171675)"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data-loaders"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# n_samples = 50\n",
    "\n",
    "batch_size = 10\n",
    "transform = \"random_swap\" #only for training\n",
    "\n",
    "train_dataset = UKBB_ROI_Dataset(train_df, pheno_cols_ses2, pheno_cols_ses3, transform=transform)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = UKBB_ROI_Dataset(test_df, pheno_cols_ses2, pheno_cols_ses3, transform=None)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# iter(train_dataloader).next()\n",
    "print(f\"len train dataset: {len(train_dataset)}, test dataset: {len(test_dataset)}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## CUDA"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    map_location=lambda storage, loc: storage.cuda()\n",
    "else:\n",
    "    map_location='cpu'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train model "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "n_epochs = 10\n",
    "input_size = len(pheno_cols_ses2)\n",
    "hidden_size = 10\n",
    "lr = 0.005\n",
    "\n",
    "# model = LSN_FF(input_size,hidden_size=hidden_size)\n",
    "model = LSN_FF_Linear(input_size,hidden_size=hidden_size)\n",
    "\n",
    "model.train()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr) #optim.SGD(model.parameters(), lr=lr, momentum=0.5)                                                                                               \n",
    "criterion = nn.MSELoss()  #nn.L1Loss() #\n",
    "\n",
    "# using subset of train dataloader for debug\n",
    "model, batch_loss_df, epoch_loss_df, preds_df = train(model,train_dataloader,optimizer,criterion,n_epochs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "plt.plot(epoch_loss_df)\n",
    "epoch_loss_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test perf"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\n",
    "test_dataloader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "eid_list, y_test_list, y_pred_list, test_loss1, test_loss2 = test(model, test_dataloader)\n",
    "y_test = np.squeeze(np.vstack(y_test_list))\n",
    "y_pred = np.squeeze(np.vstack(y_pred_list))\n",
    "\n",
    "test_r1 = stats.pearsonr(y_pred[:,0],y_test[:,0])[0]\n",
    "test_r2 = stats.pearsonr(y_pred[:,1],y_test[:,1])[0]   \n",
    "\n",
    "test_age_1 = y_test[:,0]\n",
    "test_age_2 = y_test[:,1]\n",
    "\n",
    "test_brainage_1 = y_pred[:,0] # for two timepoints y is a matrix\n",
    "test_brainage_2 = y_pred[:,1]                                    \n",
    "\n",
    "df = pd.DataFrame()\n",
    "df[\"eid\"] = eid_list\n",
    "df[\"test_age_1\"] = test_age_1\n",
    "df[\"test_age_2\"] = test_age_2\n",
    "df[\"test_brainage_1\"] = test_brainage_1\n",
    "df[\"test_brainage_2\"] = test_brainage_2\n",
    "df[\"test_loss1\"] = test_loss1                    \n",
    "df[\"test_loss2\"] = test_loss2\n",
    "df[\"test_r1\"] = test_r1\n",
    "df[\"test_r2\"] = test_r2\n",
    "\n",
    "# Test loss is L1 not MSE\n",
    "test_loss = df[\"test_loss1\"].mean()\n",
    "print(f\"test_loss: {test_loss}\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate configs for CC runs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "config_df = pd.DataFrame(columns=[\"hidden_size\",\"transform\",\"phenotype\"])\n",
    "hidden_size_list = [10,50,100]\n",
    "transform_list = [None, \"random_swap\"]\n",
    "phenotype_list = [\"CT\"] #,\"volume\",\"both\"\n",
    "\n",
    "i = 0\n",
    "for hidden_size in hidden_size_list:\n",
    "    for transform in transform_list:\n",
    "        for phenotype in phenotype_list:\n",
    "            config_df.loc[i] = [hidden_size,transform,phenotype]\n",
    "            i += 1\n",
    "\n",
    "print(config_df.shape)\n",
    "config_df"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# run_id = \"run_1\"\n",
    "# config_path = f\"../results/LSN_roi/configs/config_{run_id}.csv\"\n",
    "# config_df.to_csv(config_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot batch runs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def collate_results(file_prefix, config_id_list):\n",
    "    results_df = pd.DataFrame()\n",
    "    for config_id in config_id_list:\n",
    "        results_csv = f\"{file_prefix}_{config_id}.csv\"\n",
    "        _df = pd.read_csv(results_csv)\n",
    "        _df[\"config_id\"] = config_id\n",
    "        results_df = results_df.append(_df)\n",
    "\n",
    "    return results_df\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "run_id = \"run_3\"\n",
    "config_id_list = np.arange(6)\n",
    "\n",
    "loss_csv = f\"../results/LSN_roi/{run_id}/freesurfer_train_loss_config\"\n",
    "perf_csv = f\"../results/LSN_roi/{run_id}/freesurfer_perf_config\"\n",
    "\n",
    "loss_df = collate_results(loss_csv, config_id_list)\n",
    "perf_df = collate_results(perf_csv, config_id_list)\n",
    "\n",
    "loss_df = loss_df.rename(columns={\"Unnamed: 0\":\"epoch\"})\n",
    "\n",
    "perf_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot learning curves"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "g = sns.lineplot(x=\"epoch\", y=\"epoch_loss\", data=loss_df, hue=\"config_id\", palette=\"Set1\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Plot prediction perf"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_df = perf_df.copy()\n",
    "plot_df[\"test_loss\"] = 0.5 * (plot_df[\"test_loss1\"] + plot_df[\"test_loss2\"])\n",
    "\n",
    "print(plot_df.groupby([\"config_id\", \"visit_order\"]).mean()[\"test_loss\"])\n",
    "\n",
    "sns.set(font_scale=2)\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    g = sns.catplot(y=\"test_loss\",x=\"config_id\",hue=\"visit_order\", height=3, aspect=3, kind=\"point\", \n",
    "                    sharey=False, data=plot_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "best_configs = [2,3]\n",
    "plot_df = plot_df[plot_df[\"config_id\"].isin(best_configs)]\n",
    "\n",
    "sns.set(font_scale=2)\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    g = sns.catplot(y=\"test_loss\",x=\"config_id\",hue=\"visit_order\", height=3, aspect=3, kind=\"box\", \n",
    "                    sharey=False, data=plot_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# config 0: no data_aug, config 1: data aug\n",
    "perf_df = perf_df[(perf_df[\"config_id\"].isin(best_configs)) & (perf_df[\"visit_order\"]==\"B,F\")].copy()\n",
    "perf_df.loc[perf_df[\"config_id\"]==best_configs[0], \"data_aug\"] = False\n",
    "perf_df.loc[perf_df[\"config_id\"]==best_configs[1], \"data_aug\"] = True\n",
    "\n",
    "perf_df[\"model\"] = \"LSN\"\n",
    "perf_df[\"baseline_err\"] = perf_df[\"brainage_at_ses2\"] - perf_df[\"age_at_ses2\"]\n",
    "perf_df[\"followup_err\"] = perf_df[\"brainage_at_ses3\"] - perf_df[\"age_at_ses3\"]\n",
    "perf_df[\"brainage_delta\"] = perf_df[\"brainage_at_ses3\"] - perf_df[\"brainage_at_ses2\"]\n",
    "perf_df[\"chronoage_delta\"] = perf_df[\"age_at_ses3\"] - perf_df[\"age_at_ses2\"]\n",
    "\n",
    "perf_df[\"delta_err\"] = perf_df[\"brainage_delta\"] - perf_df[\"chronoage_delta\"]\n",
    "\n",
    "perf_df[\"Benjamin_Button\"] = perf_df[\"brainage_at_ses3\"] < perf_df[\"brainage_at_ses2\"]\n",
    "\n",
    "n_BBs = perf_df[\"Benjamin_Button\"].sum()\n",
    "print(f\"n BBs: {n_BBs} ({100*n_BBs/len(plot_df):4.3f}%)\")\n",
    "\n",
    "perf_df_melt = perf_df.melt(id_vars=[\"eid\", \"model\", \"data_aug\", \"visit_order\", \"age_at_ses2\", \"brainage_at_ses2\", \"age_at_ses3\", \"brainage_at_ses3\", \"chronoage_delta\", \"brainage_delta\"], \n",
    "              value_vars=['baseline_err', 'followup_err', 'delta_err'],\n",
    "              var_name='err_type', value_name='error')\n",
    "\n",
    "perf_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_df = perf_df_melt.copy()\n",
    "\n",
    "plot_df[\"abs_error\"] = np.abs(plot_df[\"error\"]) \n",
    "\n",
    "print(plot_df.groupby([\"err_type\",\"data_aug\"]).mean()[\"abs_error\"])\n",
    "\n",
    "sns.set(font_scale=2)\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    g = sns.catplot(x=\"err_type\",y=\"abs_error\", hue=\"data_aug\", kind=\"bar\", data=plot_df,aspect=2)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "save_summary_results = False\n",
    "if save_summary_results:\n",
    "    perf_df_melt.to_csv(f\"{summary_results_dir}LSN_model_two_visit_train_two_visit_subset_test_two_visit_subset.csv\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Legacy plots"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_df = perf_df.copy()\n",
    "plot_df = plot_df[plot_df[\"config_id\"].isin([2,3])]\n",
    "\n",
    "plot_df[\"test_MAE\"] = 0.5 * (plot_df[\"test_MAE1\"] + plot_df[\"test_MAE1\"])\n",
    "plot_df[\"brainage_diff\"] = plot_df[\"test_brainage_2\"] - plot_df[\"test_brainage_1\"]\n",
    "plot_df[\"chronoage_diff\"] = plot_df[\"test_age_2\"] - plot_df[\"test_age_1\"]\n",
    "\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    fig, ax = plt.subplots(figsize=(20,10))\n",
    "    g = sns.scatterplot(x=\"eid\",y=\"brainage_diff\",hue=\"visit_order\", style=\"config_id\",data=plot_df, ax=ax)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_df1 = plot_df[plot_df[\"visit_order\"]==\"B,F\"][[\"eid\",\"config_id\",\"brainage_diff\"]]\n",
    "plot_df2 = plot_df[plot_df[\"visit_order\"]==\"F,B\"][[\"eid\",\"config_id\",\"brainage_diff\"]]\n",
    "plot_df_long = pd.merge(plot_df1,plot_df2,on=[\"eid\",\"config_id\"])\n",
    "plot_df_long = plot_df_long.rename(columns={\"brainage_diff_x\":\"brainage_diff for B,F\", \"brainage_diff_y\":\"brainage_diff for F,B\"})\n",
    "\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    g = sns.jointplot(x=\"brainage_diff for B,F\", y=\"brainage_diff for F,B\", hue=\"config_id\", data=plot_df_long)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Vector similarity"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "perf_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.spatial import distance\n",
    "\n",
    "chrno_age = perf_df[[\"test_age_1\",\"test_age_2\"]].values\n",
    "brain_age = perf_df[[\"test_brainage_1\",\"test_brainage_2\"]].values\n",
    "\n",
    "sim_list = []\n",
    "for i in range(len(chrno_age)):\n",
    "    cos_sim = 1 - distance.cosine(chrno_age[i],brain_age[i])\n",
    "    sim_list.append(cos_sim)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_df = perf_df.copy()\n",
    "plot_df[\"cosine_sim\"] = sim_list\n",
    "\n",
    "sns.set(font_scale=2)\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    g = sns.catplot(y=\"cosine_sim\",x=\"config_id\",hue=\"visit_order\", height=3, aspect=3, kind=\"point\", \n",
    "                    sharey=False, data=plot_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sns.set(font_scale=2)\n",
    "with sns.axes_style(\"whitegrid\"):\n",
    "    g = sns.catplot(y=\"cosine_sim\",x=\"config_id\",hue=\"visit_order\", height=3, aspect=3, kind=\"box\", \n",
    "                    sharey=False, data=plot_df)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.7.9",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('green_compute': conda)"
  },
  "interpreter": {
   "hash": "e5f8cee7ddba11edeefb1347c6536a4ac2b361bd4eba89a8b32d7cb85bbef9ea"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}